# Apriori Algorithm

## Apriori Algorithm

- Level-wise search
- Candidate generation (join step)
- Pruning step
- Support counting
- Iterative process

# Apriori Algorithm

## Key Terms

| Term | Definition |
| --- | --- |
| Apriori algorithm | A level-wise algorithm for discovering frequent itemsets using the downward closure property. |
| k-itemset | An itemset containing exactly k items. |
| Candidate itemset (Ck) | The set of all possible k-itemsets that might be frequent. |
| Frequent itemset (Lk) | The set of k-itemsets whose support is at least minsup. |
| Join step | The process of creating candidate k-itemsets from frequent (k–1)-itemsets. |
| Pruning step | The elimination of candidates that contain an infrequent subset. |

---

## Level-Wise Search

Apriori performs a **level-wise search** over itemsets.

- First, it finds all frequent **1-itemsets**
- Then frequent **2-itemsets**
- Then frequent **3-itemsets**, and so on

At each level k, it uses the frequent itemsets from level k−1 to generate candidates for level k.

---

## Candidate Generation (Join Step)

Candidates $C_k$ are generated by **joining** frequent (k−1)-itemsets $L_{k-1}$.

Two (k−1)-itemsets are joined if:

- They share the first k-2 items

Example:

$\{milk, bread\} \text{ and } \{milk, butter\} \Rightarrow \{milk, bread, butter\}$

This creates all possible **k-itemsets** that might be frequent.

---

## Pruning Step

After generating candidates:

- Each candidate is checked
- If **any (k–1)-subset** of the candidate is not in $L_{k-1}$, the candidate is removed

This uses the **Apriori principle**:

> An itemset with an infrequent subset cannot be frequent.
> 

This avoids unnecessary support counting.

---

## Support Counting

For the remaining candidates:

- The database is scanned
- The number of transactions containing each candidate is counted

Support is computed as:

$support(X) = \frac{\text{count}(X)}{\text{total transactions}}$

Candidates whose support ≥ minsup become $L_k$, the frequent k-itemsets.

---

## Iterative Process

The algorithm repeats:

1. Generate candidates $C_k$ from $L_{k-1}$
2. Prune candidates
3. Count support
4. Form $L_k$

The process stops when:

- No new frequent itemsets are found

---

## Summary

Apriori efficiently discovers frequent itemsets by:

- Searching level by level
- Generating and pruning candidates
- Using downward closure to reduce computation

It is the backbone of classical association rule mining.