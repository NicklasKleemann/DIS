# K-Means

## K-Means

### Term definition table

| Term | Definition |
| --- | --- |
| K-Means | An **unsupervised clustering** algorithm that partitions data into **K clusters** by minimizing within-cluster variance. |
| Cluster | A group of similar data points. |
| Centroid | The mean (center) of all points in a cluster. |
| K | The number of clusters. |
| Inertia | Sum of squared distances between points and their cluster centroid. |
| Euclidean Distance | Distance metric commonly used in K-Means. |
| Iteration | One full cycle of assigning points and updating centroids. |
| Convergence | When cluster assignments no longer change. |
| Elbow Method | Technique to choose the best K. |

---

### Definition about the algorithm

**K-Means** groups data points into **K clusters** by repeatedly assigning each point to the nearest centroid and updating the centroids to be the mean of their assigned points.

The algorithm tries to **minimize the total squared distance** between points and their cluster centroids.

---

### Advantages / disadvantages

**Advantages**

- Simple and fast.
- Scales well to large datasets.
- Easy to implement.
- Works well for spherical, well-separated clusters.

**Disadvantages**

- Must choose K beforehand.
- Sensitive to initial centroid positions.
- Sensitive to outliers.
- Works poorly for non-spherical or overlapping clusters.

---

### Math equation

### Distance to centroid

$d(x, \mu_k) = \sqrt{\sum_{i=1}^{d} (x_i - \mu_{k,i})^2}$

### Objective function (inertia)

$J = \sum_{k=1}^{K} \sum_{x \in C_k} \|x - \mu_k\|^2$

where $C_k$ is cluster k and $\mu_k$ is its centroid.

---

### Runtime

Let:

- nnn = number of data points
- ddd = number of features
- kkk = number of clusters
- iii = number of iterations

**Training**

- Best: $O(nkd)$
- Worst: $O(nkdi)$

**Prediction**

- Best & Worst: O(kd)

---

### Python-like pseudo code

```python
defk_means(X, k, max_iters=100):
    centroids = random_points(X, k)

for _inrange(max_iters):
        clusters = [[]for _inrange(k)]

for xin X:
            idx = nearest_centroid(x, centroids)
            clusters[idx].append(x)

        new_centroids = []
for clusterin clusters:
            new_centroids.append(mean(cluster))

if new_centroids == centroids:
break

        centroids = new_centroids

return centroids, clusters
```

---

### Step-by-step through the algorithm

1. Choose K (number of clusters).
2. Randomly initialize K centroids.
3. Assign each data point to the nearest centroid.
4. Recompute each centroid as the mean of its assigned points.
5. Repeat steps 3â€“4 until centroids no longer change.
6. Output the clusters and centroids.