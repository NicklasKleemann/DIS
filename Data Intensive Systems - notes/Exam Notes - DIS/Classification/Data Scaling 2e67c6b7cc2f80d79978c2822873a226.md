# Data Scaling

- Why data scaling is needed
- Standardization
- Normalization
- StandardScaler
- MinMaxScaler
- RobustScaler
- Normalizer
- When to use each

# Data Scaling

Data scaling is a **critical preprocessing step** in classification, especially for models that use **distance or magnitude**, such as **KNN**.

The slides emphasize that **unscaled data can completely dominate a model’s behavior** if one attribute has a much larger numeric range than another.

---

## Why Data Scaling Is Needed

Many classifiers (especially **KNN**) compute distances such as:

$d(x,y) = \sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + \dots}$

If one attribute ranges from 0–10 and another from 0–10,000:

- The second attribute dominates the distance
- The first attribute becomes almost irrelevant

The slides show a KNN example where:

- Without scaling → wrong neighbors
- With scaling → correct classification

Therefore:

> Data scaling ensures that all attributes contribute fairly to distance and similarity calculations.
> 

---

## Standardization

Standardization transforms each feature so that:

- Mean = 0
- Variance = 1

Formula:

$x' = \frac{x - \mu}{\sigma}$

where:

- μ = mean
- σ = standard deviation

This makes features:

- Centered
- Comparable in scale

Used when:

- Data follows a roughly **normal distribution**

---

## Normalization

Normalization rescales each feature into a fixed range, usually:

[0,1]

Formula:

$x' = \frac{x - x_{min}}{x_{max} - x_{min}}$

Used when:

- Data is not normally distributed
- Data has unknown or skewed ranges

---

## StandardScaler

This implements **standardization**:

- Each feature ends with mean 0 and variance 1

Best when:

- Data is normally distributed
- Outliers are not extreme

---

## MinMaxScaler

This implements **normalization**:

- Each feature is mapped to [0,1]

Best when:

- You need bounded values
- The distribution is unknown

Sensitive to outliers.

---

## RobustScaler

RobustScaler:

- Uses **median** and **quartiles** instead of mean and variance
- Is less affected by outliers

Best when:

- Dataset contains extreme values
- You want stable scaling

---

## Normalizer

Normalizer scales each **data point**, not each feature.

It ensures:

$\sqrt{x_1^2 + x_2^2 + \dots} = 1$

This is useful when:

- Only the **direction** of a vector matters
- Magnitude should not affect similarity

Example:

- Text classification
- Cosine similarity

---

## How to Apply Scaling (Important!)

The slides warn:

> You must apply the same scaling to both training and test data.
> 

Correct workflow:

1. Fit scaler on the full dataset (or training set)
2. Transform training data
3. Transform test data using the same scaler
4. Then train and evaluate

---

## Standardization vs Normalization (Rule of Thumb)

From the slides:

| Situation | Use |
| --- | --- |
| Normal distribution | Standardization |
| Skewed or unknown distribution | Normalization |
| Unsure | Normalize, or standardize then normalize |
| Try multiple | Choose the one with best validation performance |

---

## Summary

Data scaling:

- Prevents dominance by large-scale attributes
- Is required for distance-based classifiers
- Directly affects classification accuracy

Choosing the right scaler is a **model selection decision** and must be validated just like the classifier.