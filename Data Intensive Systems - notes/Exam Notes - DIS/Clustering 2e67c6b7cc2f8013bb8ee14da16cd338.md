# Clustering

[Learning Paradigms](Clustering/Learning%20Paradigms%202e67c6b7cc2f807b848acc0720001172.md)

[**Clustering Fundamentals**](Clustering/Clustering%20Fundamentals%202e67c6b7cc2f806eab64c3ea4cb420a1.md)

[k-Means Clustering](Clustering/k-Means%20Clustering%202e67c6b7cc2f800091ebd2d1983b4136.md)

[Hierarchical Clustering](Clustering/Hierarchical%20Clustering%202e67c6b7cc2f80d095b1ff341350320f.md)

[**DBSCAN (Density-Based Spatial Clustering)**](Clustering/DBSCAN%20(Density-Based%20Spatial%20Clustering)%202e67c6b7cc2f8035924bf5c910d7c99f.md)

[Evaluation of Clustering](Clustering/Evaluation%20of%20Clustering%202e67c6b7cc2f8010bb69fb465b0412af.md)

[Choosing a Clustering Method](Clustering/Choosing%20a%20Clustering%20Method%202e67c6b7cc2f800e9ba0cf0bb1643e1c.md)

# **Clustering: Definitions**

> Context:Â This document aggregates all key terms, comparison tables, and logical structures from the Clustering documentation suite. It serves as a single "Cheat Sheet" for definitions.Â Parent Hub:Â Clustering Algorithm Hub
> 

---

## **1. Master Definitions Table**

| Term | Formal Definition | Spotify "Golden Thread" Analogy |
| --- | --- | --- |
| **Cluster** | A subset of objects where intra-cluster distance is minimized and inter-cluster distance is maximized. | A specific playlist (e.g., "Heavy Metal") where all songs sound alike. |
| **Centroid** | The geometric mean (average) of all points in a cluster. | The "perfect average song" of a genre (does not have to be a real song). |
| **Outlier (Noise)** | A point that does not fit well into any cluster. | A weird avant-garde track that sounds like a lawnmower. |
| **Dendrogram** | A tree diagram showing the taxonomic relationship between clusters. | A "Family Tree" of music genres (Music -> Rock -> Punk). |
| **SSE** | Sum of Squared Error (Distance from points to their centroid). | How "messy" a playlist is. Low SSE = very consistent vibe. |
| **Core Point** | A point withÂ â‰¥â‰¥Â MinPts neighbors within radiusÂ Ïµ*Ïµ*. | The "Hit Song" that defines the genre. |
| **Border Point** | A point with < MinPts neighbors but within reach of a Core point. | The "Bridge Song" or B-side that loosely fits the genre. |
| **K-Means** | Partitioning algorithm that minimizes variance (SSE) usingÂ K*K*Â centroids. | Moving 3 pins around the map until they settle in the center of 3 genre clouds. |
| **Hierarchical** | Agglomerative algorithm building a nested tree structure. | Merging songs pair-by-pair to reconstruct the history of music genres. |
| **DBSCAN** | Density-based algorithm that finds arbitrary shapes and isolates noise. | Finding strict "underground scenes" and ignoring mainstream radio noise. |

---

## **2. Fundamental Comparisons**

### **Classification vs. Clustering**

| Feature | Classification (Supervised) | Clustering (Unsupervised) |
| --- | --- | --- |
| **Labels** | **Pre-defined**Â (e.g., "Spam" vs "Accurate"). | **None**Â (We discover labels). |
| **Training** | **Yes**Â (Train on labeled data). | **No**Â (Algorithm explores data directly). |
| **Goal** | Assign new object toÂ *existing*Â class. | Group objects intoÂ *new*Â classes. |
| **Spotify Ex** | "Is this song Explicit? (Yes/No)" | "What genres exist in this database?" |

### **Inter-cluster vs. Intra-cluster Use**

| Goal | Description | Spotify Example |
| --- | --- | --- |
| **Intra-cluster Similarity**Â (Cohesion) | ItemsÂ *inside*Â a cluster should be very similar. | All songs in the "Metal" playlist should be loud and fast. |
| **Inter-cluster Similarity**Â (Separation) | Clusters should be distinct/far apart fromÂ *other*Â clusters. | "Metal" playlist should sound very different from "Lullaby" playlist. |

---

## **3. Algorithm Selection Matrix**

Which algorithm should you use?

| Feature | **K-Means** | **Hierarchical** | **DBSCAN** |
| --- | --- | --- | --- |
| **Input Required** | KÂ (Number of clusters) | None (Cut tree later) | ÏµÂ (Radius), MinPts |
| **Shape Limit** | **Spherical**Â (Blobs) only | Depends on Linkage | **Arbitrary**Â (Rings, snakes) |
| **Outliers** | **Sensitive**Â (Bad) | Can handle (depends on linkage) | **Great**Â (Marks as Noise) |
| **Complexity** | **Fast**Â $(O(N))$ | **Slow**Â $(O(N2))$ | **Medium**Â $(O(Nlogâ¡N))$ |
| **Best For...** | General partitioning, known K | Taxonomies, small data | Noisy data, complex shapes |

---

## **4. Specific Algorithm Details**

### **K-Means: Weaknesses & Fixes**

| Weakness | Description | The Fix |
| --- | --- | --- |
| **Local Optima** | If you pick bad starting points, you get bad clusters. | **Run Multiple Times:**Â Run it 10 times with different random starts and pick the best one. |
| **Outliers** | One "Lawnmower Sound" (Outlier) can pull the "Pop" centroid miles away. | **Pre-process:**Â Remove outliers before running K-Means. |
| **Choosing K** | We often don't know if there are 3 genres or 5. | **Elbow Method:**Â Plot SSE vs K and find the bend. |

### **Hierarchical: Linkage Criteria**

| Linkage Type | Definition | Effect/Shape |
| --- | --- | --- |
| **Single Link**Â (Min) | Distance between theÂ **closest**Â two points. | **"Chain" Effect:**Â Good for snake-like shapes. Sensitive to noise. |
| **Complete Link**Â (Max) | Distance between theÂ **furthest**Â two points. | **Compact Spheres:**Â Forces tight, round clusters. |
| **Average Link** | Average distance between all pairs. | **Balanced:**Â Compromise between Single and Complete. |
| **Ward's Method** | Merge pair minimizingÂ **SSE**Â increase. | **Compact:**Â Most common, behaves like K-Means. |

### **DBSCAN: Point Classification**

| Point Type | Condition | Role |
| --- | --- | --- |
| **Core Point** | NeighborsÂ â‰¥Â MinPts withinÂ *Ïµ*. | The dense interior of a cluster.Start of a new group. |
| **Border Point** | Neighbors < MinPts, but neighbor of Core. | The edge of the cluster. Assigned to the Core's group. |
| **Noise Point** | Neither Core nor Border. | **Discarded**. Does not belong to any cluster. |

---

## **5. Evaluation Metrics**

| Metric | Formula/Logic | Interpretation |
| --- | --- | --- |
| **Silhouette Score** |  $S = \frac{b - a}{\max(a, b)}$
$a$: Cohesion
$b$: Separation | **+1:**Â Perfect.
**0:**Â Overlapping.
**-1:**Â Wrong. |
| **SSE (Sum Sq Error)** | Sum of squared dists to centroid. | **Lower is better**. Used for Elbow Method. |
| **Rand Index (RI)** | $Accuracy = \frac{TP + TN}{All Pairs}$ | **0.0 - 1.0**. Measures agreement with ground truth. |
| **Purity** | % of dominant class in cluster. | Hard to use (Making 1 cluster per point gives 100% purity). |

# **Clustering: Spotify Song Segmentation**

## **1. System Map & Context**

- **Context:**Â Part ofÂ **Unsupervised Learning**Â (Machine Learning). Used when we haveÂ *data*Â butÂ *no labels*Â (no specific "correct answer" to train on).
- **Input:**Â Raw data objects (e.g., Songs, Customers, Sensors).
- **Output:**Â Disjoint groups (Clusters) where items areÂ *similar*Â to each other andÂ *dissimilar*Â to others.
- ðŸ”—Â **Dependency:**Â Relates toÂ **Big Data "Variety"**Â (seeÂ [Foundation of datasystems](Foundation%20of%20Data%20Systems%202e37c6b7cc2f805bb00dda7f622ed3a8.md)): handling unstructured/complex data often requires clustering to find structure.

> The Golden Thread: Spotify Song SegmentationÂ Throughout this document, we are building aÂ Spotify Auto-Categorizer.
> 
> - **Goal:**Â Automatically group millions of songs into "Mood Playlists" (e.g., "Chill", "Workout", "Party") without human inputs.
> - **Data Points (P*P*):**Â Each song is a 2D point:
>     - x:Â **Tempo**Â (0-200 BPM)
>     - y:Â **Energy**Â (0.0 Low - 1.0 High)

---

## **2. Definitions & Goals**

> ðŸ‘‰Â Deep Dive: [Clustering Fundamentals](Clustering/Clustering%20Fundamentals%202e67c6b7cc2f806eab64c3ea4cb420a1.md)
> 

### **Classification vs. Clustering (Exam Critical)**

| Feature | Classification (Supervised) | Clustering (Unsupervised) |
| --- | --- | --- |
| **Labels** | **Pre-defined**Â (e.g., "Spam" vs "Accurate"). | **None**Â (We discover labels). |
| **Training** | **Yes**Â (Train on labeled data). | **No**Â (Algorithm explores data directly). |
| **Goal** | Assign new object toÂ *existing*Â class. | Group objects intoÂ *new*Â classes. |
| **Spotify Ex** | "Is this song Explicit? (Yes/No)" | "What genres exist in this database?" |

### **Definitions Table**

| Term | Formal Definition | ELI5 (Spotify Example) |
| --- | --- | --- |
| **Cluster** | A subset of objects where intra-cluster distance is minimized and inter-cluster distance is maximized. | A specific playlist (e.g., "Heavy Metal") where all songs sound alike. |
| **Centroid** | The geometric mean (average) of all points in a cluster. | The "perfect average song" of a genre (does not have to be a real song). |
| **Outlier** | A point that does not fit well into any cluster. | A weird avant-garde track that sounds like a lawnmower. |
| **Dendrogram** | A tree diagram showing the taxonomic relationship between clusters. | A "Family Tree" of music genres (Music -> Rock -> Punk). |
| **SSE** | Sum of Squared Error (Distance from points to their centroid). | How "messy" a playlist is. Low SSE = very consistent vibe. |

### **System Goals**

| Goal | Description | Trade-off |
| --- | --- | --- |
| **Cohesion** | Items inside a cluster should be very similar. | **Overfitting:**Â If too cohesive, you get 1,000 tiny clusters (1 playlist per song). |
| **Separation** | Clusters should be distinct/far apart. | **Generalization:**Â If too separated, you might merge distinct genres (Rock + Metal) into one. |
| **Interpretability** | Clusters must make business sense. | **Complexity:**Â Mathematical optimals (weird shapes) might not be human-readable. |

---

## **3. Topic Walkthrough**

### **A. Preprocessing (The Foundation)**

**The Problem:**Â Raw data often has different scales. Tempo is 0-200, Energy is 0-1.Â **The Naive Approach:**Â Calculate Euclidean distance directly.

- *Fail:*Â A change of 1.0 in Energy is massive (0% to 100%), but numeric distance is just "1". A change of 1 BPM is tiny, but numeric distance is also "1". Tempo dominates the distance; Energy is ignored.Â **The Solution:**Â **Scaling**Â (Normalization/Standardization).
- **Min-Max Scaling:**Â Squishes everything to [0, 1]. (Good for image pixel data).
- **Standard Scaler (Z-Score):**Â Centers around 0 with Variance=1. (Good for Gaussian data).
- *Golden Thread:*Â Scale both Tempo and Energy to a 0-1 range so they contribute equally.

---

### **B. K-Means Clustering for Playlists**

> ðŸ‘‰Â Deep Dive: [K-Means Algorithm & Elbow Method](Clustering/k-Means%20Clustering%202e67c6b7cc2f800091ebd2d1983b4136.md)
> 

**The Concept Logic**

- **The Problem:**Â We want to partitionÂ *N*Â songs into exactlyÂ *K*Â playlists.
- **The Solution:**Â Iteratively moveÂ *K*Â centers until they stabilize.
- **Golden Thread:**Â PlaceÂ *K*=3Â random dots on the Tempo/Energy graph. Move them until they sit in the middle of "Pop", "Rock", and "Jazz".

**The Decision Tree**

> If you knowÂ KKÂ (or can guess it) AND data is globular (spherical blobs)Â â†’â†’Â UseÂ K-Means. If data has arbitrary shapes (Crescents, Rings)Â â†’â†’Â Do NOTÂ use K-Means (Use DBSCAN).
> 

**The Algorithm (Step-by-Step Solver)**

1. **Pick K:**Â Decide number of clusters (e.g.,Â *K*=2).
2. **Initialize:**Â PickÂ *K*Â random points as initial centroids (*C*1,*C*2).
3. **Assign:**Â For every song pointÂ *Pi*:
    
    Pi
    
    - Calculate distance toÂ $*C_1*$:Â $d_1 = \sqrt{(x_i - x_{c1})^2 + (y_i - y_{c1})^2}$
    - Calculate distance toÂ $*C_2*$:Â *d*2=â€¦
    - AssignÂ *Pi*Â to closest Centroid.
4. **Update:**Â Recalculate Centroids.
    
    $New C_1 = (\text{Average } x \text{ of all points in } C_1, \text{Average } y \text{ of points})$.
    
5. **Repeat:**Â Steps 3-4 until centroids stop moving (Convergence).

**Weaknesses (Exam Warnings)**

- **Local Optima:**Â Bad random initialization = Bad results. (Fix: Run multiple times).
- **Outliers:**Â Mean is sensitive to outliers. One "Lawnmower Song" pulls the whole Pop centroid away.

**Elbow Method (Finding K)**

- PlotÂ **SSE**Â (y-axis) vsÂ **K**Â (x-axis).
- Look for the "Elbow" where the drop flattens out.
- *Logic:*Â Adding more clusters always reduces error, but after the elbow, you're just splitting coherent groups for tiny gains.

---

### **C. Hierarchical Clustering (The Taxonomy)**

> ðŸ‘‰Â Deep Dive: [Hierarchical & Dendrograms](Clustering/Hierarchical%20Clustering%202e67c6b7cc2f80d095b1ff341350320f.md)
> 

**The Concept Logic**

- **The Problem:**Â We don't knowÂ *K*, and we want a structure (Genres have Sub-genres).
- **The Solution:**Â **Agglomerative Clustering**Â (Bottom-Up). Start withÂ *N*Â clusters (every song is a cluster) and merge the closest pair.
- **Golden Thread:**Â Merge "Song A" and "Song B" -> "Pop Cluster". Merge "Pop Cluster" and "Rock Cluster" -> "Mainstream Cluster".

**The Decision Tree**

> If you need a hierarchy/tree structureÂ â†’Â UseÂ Hierarchical. If dataset is massiveÂ â†’Â AvoidÂ (Complexity is $O(N^3)$ or $O(N^2)$, slow)
> 

**Likage Criteria (Measuring Distance Between Clusters)**Â When merging Cluster A (Pop) and Cluster B (Rock), how do we measure distance?

1. **Single Link (Min):**Â Distance between theÂ **closest**Â two points.
    - *Effect:*Â Creates long, "chain-like" clusters. Good for non-spherical shapes. Sensitive to noise.
2. **Complete Link (Max):**Â Distance between theÂ **furthest**Â two points.
    - *Effect:*Â Creates tight, compact spherical clusters.
3. **Average Link:**Â Average distance between all pairs.
4. **Ward's Method:**Â Merges the pair that minimizes the increase inÂ **SSE**Â (Variance).
    - *Effect:*Â Creates very compact, similar-sized clusters (like K-Means but hierarchical). Default choice in many libraries.

**The Algorithm (Dendrogram Reading)**

1. **Construct Matrix:**Â Calculate distance between ALl pairs.
2. **Merge:**Â Find smallest distance. Combine into new cluster.
3. **Update Matrix:**Â Recalculate distance from New Cluster to all existing clusters.
4. **Repeat:**Â Until 1 giant cluster remains.
5. **Cut:**Â Draw a horizontal line on the Dendrogram to choose specificÂ *K*.
    
    K
    
    - *Exam Tip:*Â The "Best" K is usually where the vertical lines crossed are longest (largest gap between merges).

---

### **D. DBSCAN (The Niche Finder)**

> ðŸ‘‰Â Deep Dive: [DBSCAN & Density](Clustering/DBSCAN%20(Density-Based%20Spatial%20Clustering)%202e67c6b7cc2f8035924bf5c910d7c99f.md)
> 

**The Concept Logic**

- **The Problem:**Â K-Means fails on "weird shapes" (e.g., a "Ring" of songs surrounding a central genre). It also forces outliers into clusters.
- **The Solution:**Â Density-Based Spatial Clustering. Clusters are regions of high density.
- **Golden Thread:**Â Identify a "Mainstream Pop" core. If a song is close enough, it's in. If a song is far away from everyone, it'sÂ **Noise**Â (ignore it).

**The Decision Tree**

> If you have noise/outliersÂ â†’Â UseÂ DBSCAN. If clusters have irregular shapes (not circles)Â â†’Â UseÂ DBSCAN. If density varies strictly (some clusters sparse, some dense)Â â†’Â DBSCAN struggles.
> 

**Key Parameters**

- **Ïµ*Ïµ*Â (Eps):**Â The "Reach" radius. How close considers "neighbors"?
- **MinPts:**Â Minimum neighbors to consider a "Core" point.

**Point Types**

1. **Core Point:**Â HasÂ â‰¥Â MinPts neighbors within radiusÂ *Ïµ*. (The "Hit Song").
2. **Border Point:**Â Has < MinPts neighbors, but belongs to a Core point's neighborhood. (The "Bridge" song).
3. **Noise:**Â Neither Core nor Border. (The "Garbage").

**Reachability vs Connectivity (Exam Nuance)**

- **Directly Density-Reachable:**Â *A*â†’*B*Â (A is Core, B is in A's circle).Â *Not symmetric*Â (B might not be Core).
- **Density-Reachable:**Â Chain of direct reaches ($*Aâ†’Câ†’D*$).
- **Density-Connected:**Â $*Bâ†Aâ†’C*$. B and C are connected because they share a Core ancestor A.Â *Symmetric*. (**Rule:**Â Clusters form based on this).

**The Algorithm (Step-by-Step)**

1. Pick an unvisited pointÂ *P*.
2. Count neighbors withinÂ *Ïµ*Â distance.
3. **If Neighbors < MinPts:**Â Mark asÂ **Noise**Â (might change later).
4. **If NeighborsÂ â‰¥Â MinPts:**Â Mark asÂ **Core**. Start a New Cluster.
    - Add all neighbors to cluster.
    - Recursively checkÂ *their*Â neighbors (if they are also Core, expand cluster).
5. Repeat until all points visited.

---

## **4. Comparisons & Trade-offs**

### **Algorithm Comparison**

| Feature | K-Means | Hierarchical | DBSCAN |
| --- | --- | --- | --- |
| **Input Required** | KÂ (Number of clusters) | None (Cut tree later) | *Ïµ*, MinPts |
| **Shape Limit** | Spherical (Convex) only | Depends on Linkage | Arbitrary shapes |
| **Outliers** | Forces them into clusters (Bad) | Can handle (depends) | **Great**Â (Marks as Noise) |
| **Complexity** | Fast $(O(N))$ | Slow $(O(N2))$ | Medium ($O(N\logâ¡N))$ |
| **Best For...** | General purpose, simple partitioning | Taxonomies, small data | Noisy data, complex shapes |

### **Evaluation Metrics (Is the playlist good?)**

| Metric | Formula/Logic | Interpretation |
| --- | --- | --- |
| **Silhouette Score** | $S = \frac{b - a}{\max(a, b)}$
$*a*$: Cohesion,Â 
$*b*$: Separation | **+1:**Â Perfect.
**0:**Â Overlapping.
**-1:**Â Wrong. |
| **SSE (Sum Sq Error)** | Sum of squared dists to centroid. | **Lower is better**. Used for Elbow Method. |
| **Rand Index (RI)** | $Accuracy = \frac{TP + TN}{All Pairs}$ | **0 - 1**. Measures agreement with ground truth. |
| **Adjusted RI (ARI)** | Corrected for random chance. | **0.0:**Â Random guessing (Bad).
**1.0:**Â Perfect Match. |
| **Purity** | % of dominant class in cluster. | Hard to use (Making 1 cluster per point gives 100% purity). |

---

## **5. Exam Checklist (The "Cheat Code")**

1. **Preprocessing?**Â Always check if data needs Normalization (Scaling) first.
2. **Choosing Algo?**
    - Arbitrary Shapes/Noise?Â â†’Â **DBSCAN**.
    - Taxonomy needed?Â â†’Â **Hierarchical**.
    - FixedÂ *K*Â / Speed needed?Â â†’Â **K-Means**.
3. **Dendo-Cut?**Â To getÂ *K*Â clusters from a Dendrogram, draw a horizontal line that intersectsÂ *K*Â vertical lines. Look for theÂ *longest*Â vertical drop.
4. **K-Means Update?**Â New Centroid is simply the averageÂ (*x*,*y*)Â of all current points.